MLX: Failed to load symbol: mlx_metal_device_info
Couldn't find '/Users/rohitchandra/.ollama/id_ed25519'. Generating new private key.
Your new public key is: 

ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIHYKKeG3OA9cp370jqrBC+xbRa/X+S+byNe4cq9mJ7CC

time=2026-02-07T17:02:56.900+05:30 level=INFO source=routes.go:1636 msg="server config" env="map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:0 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/Users/rohitchandra/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false http_proxy: https_proxy: no_proxy:]"
time=2026-02-07T17:02:56.901+05:30 level=INFO source=images.go:473 msg="total blobs: 0"
time=2026-02-07T17:02:56.901+05:30 level=INFO source=images.go:480 msg="total unused blobs removed: 0"
time=2026-02-07T17:02:56.902+05:30 level=INFO source=routes.go:1689 msg="Listening on 127.0.0.1:11434 (version 0.15.5)"
time=2026-02-07T17:02:56.902+05:30 level=INFO source=runner.go:67 msg="discovering available GPUs..."
time=2026-02-07T17:02:56.905+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --ollama-engine --port 54042"
time=2026-02-07T17:03:07.455+05:30 level=INFO source=types.go:42 msg="inference compute" id=0 filter_id=0 library=Metal compute=0.0 name=Metal description="Apple M3 Max" libdirs="" driver=0.0 pci_id="" type=discrete total="107.5 GiB" available="107.5 GiB"
time=2026-02-07T17:03:07.455+05:30 level=INFO source=routes.go:1739 msg="vram-based default context" total_vram="107.5 GiB" default_num_ctx=262144
[GIN] 2026/02/07 - 17:03:07 | 200 |     132.542µs |       127.0.0.1 | HEAD     "/"
time=2026-02-07T17:03:09.691+05:30 level=INFO source=download.go:179 msg="downloading 0bd51f8f0c97 in 40 1 GB part(s)"
time=2026-02-07T17:17:49.176+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 9 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2026-02-07T17:23:41.111+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 6 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2026-02-07T17:30:22.881+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 28 attempt 0 failed: unexpected EOF, retrying in 1s"
[GIN] 2026/02/07 - 17:55:55 | 200 |        52m47s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2026/02/07 - 20:24:36 | 200 |        18.5µs |       127.0.0.1 | HEAD     "/"
time=2026-02-07T20:24:42.892+05:30 level=INFO source=download.go:179 msg="downloading 0bd51f8f0c97 in 40 1 GB part(s)"
time=2026-02-07T20:33:26.516+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 26 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2026-02-07T20:44:39.385+05:30 level=INFO source=download.go:376 msg="0bd51f8f0c97 part 39 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2026-02-07T20:44:39.385+05:30 level=INFO source=download.go:376 msg="0bd51f8f0c97 part 30 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2026-02-07T20:44:39.385+05:30 level=INFO source=download.go:376 msg="0bd51f8f0c97 part 32 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2026-02-07T20:44:39.388+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 39 attempt 0 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 1s"
time=2026-02-07T20:44:39.388+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 32 attempt 0 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 1s"
time=2026-02-07T20:44:39.388+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 30 attempt 0 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 1s"
time=2026-02-07T20:44:40.384+05:30 level=INFO source=download.go:376 msg="0bd51f8f0c97 part 33 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2026-02-07T20:44:40.384+05:30 level=INFO source=download.go:376 msg="0bd51f8f0c97 part 29 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2026-02-07T20:44:40.386+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 33 attempt 0 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 1s"
time=2026-02-07T20:44:40.386+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 29 attempt 0 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 1s"
time=2026-02-07T20:44:40.390+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 30 attempt 1 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 2s"
time=2026-02-07T20:44:40.390+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 32 attempt 1 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 2s"
time=2026-02-07T20:44:40.390+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 39 attempt 1 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 2s"
time=2026-02-07T20:44:41.388+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 33 attempt 1 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 2s"
time=2026-02-07T20:44:41.388+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 29 attempt 1 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 2s"
time=2026-02-07T20:44:42.392+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 32 attempt 2 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 4s"
time=2026-02-07T20:44:42.392+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 30 attempt 2 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 4s"
time=2026-02-07T20:44:42.392+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 39 attempt 2 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 4s"
time=2026-02-07T20:44:43.391+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 29 attempt 2 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 4s"
time=2026-02-07T20:44:43.391+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 33 attempt 2 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 4s"
time=2026-02-07T20:44:46.395+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 30 attempt 3 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 8s"
time=2026-02-07T20:44:46.395+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 32 attempt 3 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 8s"
time=2026-02-07T20:44:46.395+05:30 level=INFO source=download.go:297 msg="0bd51f8f0c97 part 39 attempt 3 failed: Get \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/0b/0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20260207%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20260207T145443Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f06d1cecb1ef1107598039357a18a89bf955cae10751c68205b3384936266bc2\": dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com: no such host, retrying in 8s"
time=2026-02-07T20:59:05.394+05:30 level=INFO source=download.go:376 msg="0bd51f8f0c97 part 39 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2026-02-07T21:01:15.596+05:30 level=INFO source=images.go:848 msg="request failed: Head \"https://registry.ollama.ai/v2/library/llama3/blobs/sha256:4fa551d4f938f68b8c1e6afa9d28befb70e3f33f75d0753248d530364aeea40f\": read tcp [2409:4091:3f:f404:e867:828a:ff06:d180]:55657->[2606:4700:3036::6815:4be3]:443: read: connection reset by peer"
[GIN] 2026/02/07 - 21:01:15 | 200 |        36m38s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2026/02/07 - 21:04:21 | 200 |      22.542µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 21:04:21 | 200 |     164.625µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2026/02/07 - 21:04:41 | 200 |      23.417µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 21:04:41 | 200 |      84.041µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2026/02/07 - 21:04:47 | 200 |      15.917µs |       127.0.0.1 | HEAD     "/"
time=2026-02-07T21:04:52.266+05:30 level=INFO source=download.go:179 msg="downloading 4fa551d4f938 in 1 12 KB part(s)"
time=2026-02-07T21:04:54.594+05:30 level=INFO source=download.go:179 msg="downloading 8ab4849b038c in 1 254 B part(s)"
time=2026-02-07T21:04:56.350+05:30 level=INFO source=download.go:179 msg="downloading 577073ffcc6c in 1 110 B part(s)"
time=2026-02-07T21:04:58.409+05:30 level=INFO source=download.go:179 msg="downloading ea8e06d28e47 in 1 486 B part(s)"
[GIN] 2026/02/07 - 21:05:01 | 200 | 13.864953916s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2026/02/07 - 21:05:01 | 200 |      20.209µs |       127.0.0.1 | HEAD     "/"
time=2026-02-07T21:05:04.168+05:30 level=INFO source=download.go:179 msg="downloading 633fc5be925f in 16 136 MB part(s)"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 9.057 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-70B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 80
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type q4_0:  561 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 37.22 GiB (4.53 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta-Llama-3-70B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-07T21:05:48.355+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=8192
time=2026-02-07T21:05:48.356+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d --port 55761"
time=2026-02-07T21:05:48.360+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="126.0 GiB" free_swap="0 B"
time=2026-02-07T21:05:48.360+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-07T21:05:48.360+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=81 requested=-1
time=2026-02-07T21:05:48.361+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="36.7 GiB"
time=2026-02-07T21:05:48.361+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="2.5 GiB"
time=2026-02-07T21:05:48.361+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="1.1 GiB"
time=2026-02-07T21:05:48.361+05:30 level=INFO source=device.go:272 msg="total memory" size="40.2 GiB"
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-07T21:05:48.425+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 9.131 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-07T21:05:48.425+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-07T21:05:57.591+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:55761"
time=2026-02-07T21:05:57.593+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:8192 KvCacheType: NumThreads:12 GPULayers:81[ID:0 Layers:81(0..80)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-07T21:05:57.593+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T21:05:57.593+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-70B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 80
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type q4_0:  561 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 37.22 GiB (4.53 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 8192
print_info: n_embd_inp       = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta-Llama-3-70B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
load_tensors: Metal_Mapped model buffer size = 37546.98 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 8192
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.52 MiB
llama_kv_cache:      Metal KV buffer size =  2560.00 MiB
llama_kv_cache: size = 2560.00 MiB (  8192 cells,  80 layers,  1/1 seqs), K (f16): 1280.00 MiB, V (f16): 1280.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   266.50 MiB
llama_context:        CPU compute buffer size =    32.01 MiB
llama_context: graph nodes  = 2487
llama_context: graph splits = 2
time=2026-02-07T21:06:00.601+05:30 level=INFO source=server.go:1387 msg="llama runner started in 12.24 seconds"
time=2026-02-07T21:06:00.602+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-07T21:06:00.602+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T21:06:00.602+05:30 level=INFO source=server.go:1387 msg="llama runner started in 12.24 seconds"
time=2026-02-07T21:08:07.189+05:30 level=INFO source=download.go:179 msg="downloading fa8235e5b48f in 1 1.1 KB part(s)"
time=2026-02-07T21:08:08.917+05:30 level=INFO source=download.go:179 msg="downloading 542b217f179c in 1 148 B part(s)"
time=2026-02-07T21:08:10.619+05:30 level=INFO source=download.go:179 msg="downloading 8dde1baf1db0 in 1 78 B part(s)"
time=2026-02-07T21:08:12.324+05:30 level=INFO source=download.go:179 msg="downloading 23291dc44752 in 1 483 B part(s)"
[GIN] 2026/02/07 - 21:08:14 | 200 |         3m13s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2026/02/07 - 21:10:08 | 200 |         4m29s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:10:12 | 200 | 39.179320917s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/02/07 - 21:12:36 | 200 |         2m27s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:15:37 | 200 |          3m1s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:19:06 | 200 |         3m29s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:21:32.000+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
time=2026-02-07T21:21:32.000+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
time=2026-02-07T21:21:32.000+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:21:32 | 500 | 38.739977084s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:21:32.000+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:21:32 | 500 | 38.740049042s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:21:32 | 500 | 38.738156375s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:21:32.000+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:21:32 | 500 |  38.74004425s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:21:32 | 500 |  38.73864875s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:21:32.000+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
time=2026-02-07T21:21:32.000+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:21:32 | 500 | 38.739334458s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:21:32.000+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:21:32 | 500 | 38.738795166s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:21:32 | 500 | 38.739158208s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:22:27.556+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
time=2026-02-07T21:22:27.556+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
time=2026-02-07T21:22:27.556+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
time=2026-02-07T21:22:27.556+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
time=2026-02-07T21:22:27.556+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:22:27 | 500 | 23.801860958s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:22:27.556+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:22:27 | 500 | 23.804829708s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:22:27.556+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:22:27 | 500 | 23.802778416s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:22:27 | 500 |  23.80369825s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:22:27 | 500 | 23.801754666s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:22:27 | 500 | 23.805189291s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:22:27.556+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:22:27 | 500 | 23.802805625s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:22:27 | 500 | 23.805149667s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:22:28 | 200 |         3m22s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:23:07.863+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
time=2026-02-07T21:23:07.863+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
time=2026-02-07T21:23:07.863+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
time=2026-02-07T21:23:07.863+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
time=2026-02-07T21:23:07.863+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
time=2026-02-07T21:23:07.863+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
time=2026-02-07T21:23:07.863+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:23:07 | 500 | 29.774720334s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:23:07 | 500 | 29.774777958s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:23:07.863+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:23:07 | 500 | 29.772918125s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:23:07 | 500 |    29.773052s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:23:07 | 500 | 29.773204916s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:23:07 | 500 | 29.774685542s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:23:07 | 500 | 29.774490792s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:23:07 | 500 | 29.773437084s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:23:15 | 200 | 46.673648209s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:24:41.741+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
time=2026-02-07T21:24:41.741+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:24:41 | 200 |         1m18s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:24:41.741+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:24:41 | 500 |         1m18s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:24:41.741+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:24:41 | 500 |         1m18s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:24:41.741+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:24:41 | 500 |         1m18s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:24:41.741+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:24:41 | 500 |         1m18s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:24:41.741+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:24:41 | 500 |         1m18s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:24:41 | 500 |         1m18s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:24:41 | 500 |         1m18s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:25:42 | 200 | 51.861059875s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:25:42.163+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:25:42 | 500 | 51.861325541s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:25:50.487+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="67.3 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2026-02-07T21:25:50.514+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-07T21:25:50.515+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --port 57470"
time=2026-02-07T21:25:50.520+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="50.2 GiB" free_swap="0 B"
time=2026-02-07T21:25:50.520+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="66.8 GiB" free="67.3 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-07T21:25:50.520+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-07T21:25:50.520+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="2.0 GiB"
time=2026-02-07T21:25:50.520+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="48.0 GiB"
time=2026-02-07T21:25:50.520+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="8.0 GiB"
time=2026-02-07T21:25:50.520+05:30 level=INFO source=device.go:272 msg="total memory" size="58.0 GiB"
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-07T21:25:50.590+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 11.361 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-07T21:25:50.591+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-07T21:26:01.995+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:57470"
time=2026-02-07T21:26:01.998+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-07T21:26:01.999+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T21:26:01.999+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_embd_inp       = 3072
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 96
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 96
print_info: n_embd_head_v    = 96
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 3072
print_info: n_embd_v_gqa     = 3072
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 4096
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    52.84 MiB
load_tensors: Metal_Mapped model buffer size =  2021.82 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.13 MiB
llama_kv_cache:      Metal KV buffer size = 49152.00 MiB
llama_kv_cache: size = 49152.00 MiB (131072 cells,  32 layers,  1/1 seqs), K (f16): 24576.00 MiB, V (f16): 24576.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   396.01 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 935
llama_context: graph splits = 2
time=2026-02-07T21:26:10.532+05:30 level=INFO source=server.go:1387 msg="llama runner started in 20.01 seconds"
time=2026-02-07T21:26:10.532+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=2
time=2026-02-07T21:26:10.532+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T21:26:10.532+05:30 level=INFO source=server.go:1387 msg="llama runner started in 20.01 seconds"
time=2026-02-07T21:26:28.162+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:26:28 | 200 |    37.692923s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:26:28.162+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:26:28 | 500 |   37.6930535s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:26:28.162+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:26:28 | 500 | 37.693567333s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:26:28 | 500 | 37.693615833s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:26:48 | 200 | 11.282256958s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:26:52 | 200 |  2.768345542s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:26:54 | 200 |  2.038588916s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:26:56 | 200 |    1.7230385s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:27:22 | 200 |    18.902637s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:27:28 | 200 | 30.588177459s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:27:45 | 200 |  15.38419575s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:27:48 | 200 |  2.647776667s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:27:59 | 200 |  9.528756125s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:28:04 | 200 | 16.167744458s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:28:06 | 200 |  6.264463417s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:28:15 | 200 |  8.448015291s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:28:24 | 200 |  8.497509125s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:28:44 | 200 | 19.474364125s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:29:29 | 200 |          1m3s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:30:34 | 200 |          1m3s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:31:56 | 200 |         3m10s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:31:56 | 200 |         1m20s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:32:14 | 200 | 17.406360083s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:32:46 | 200 | 30.504360834s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:33:17 | 200 | 29.796014916s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:33:28 | 200 |  8.922244958s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:33:37 | 200 |  8.688934459s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:33:48 | 200 |  9.064743334s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T21:41:35.646+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="49.5 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-70B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 80
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type q4_0:  561 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 37.22 GiB (4.53 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta-Llama-3-70B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-07T21:41:36.338+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=8192
time=2026-02-07T21:41:36.340+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d --port 58797"
time=2026-02-07T21:41:36.358+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="42.0 GiB" free_swap="0 B"
time=2026-02-07T21:41:36.358+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="49.0 GiB" free="49.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-07T21:41:36.358+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=81 requested=-1
time=2026-02-07T21:41:36.362+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="36.7 GiB"
time=2026-02-07T21:41:36.362+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="2.5 GiB"
time=2026-02-07T21:41:36.363+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="1.1 GiB"
time=2026-02-07T21:41:36.363+05:30 level=INFO source=device.go:272 msg="total memory" size="40.2 GiB"
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-07T21:41:36.511+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.028 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-07T21:41:36.512+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-07T21:41:36.659+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:58797"
time=2026-02-07T21:41:36.668+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:8192 KvCacheType: NumThreads:12 GPULayers:81[ID:0 Layers:81(0..80)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-07T21:41:36.669+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T21:41:36.670+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-70B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 80
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type q4_0:  561 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 37.22 GiB (4.53 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 8192
print_info: n_embd_inp       = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta-Llama-3-70B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
load_tensors: Metal_Mapped model buffer size = 37546.98 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 8192
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.52 MiB
llama_kv_cache:      Metal KV buffer size =  2560.00 MiB
llama_kv_cache: size = 2560.00 MiB (  8192 cells,  80 layers,  1/1 seqs), K (f16): 1280.00 MiB, V (f16): 1280.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   266.50 MiB
llama_context:        CPU compute buffer size =    32.01 MiB
llama_context: graph nodes  = 2487
llama_context: graph splits = 2
time=2026-02-07T21:41:52.782+05:30 level=INFO source=server.go:1387 msg="llama runner started in 16.42 seconds"
time=2026-02-07T21:41:52.782+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=2
time=2026-02-07T21:41:52.782+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T21:41:52.783+05:30 level=INFO source=server.go:1387 msg="llama runner started in 16.43 seconds"
time=2026-02-07T21:42:03.961+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 21:42:03 | 500 | 18.408466125s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:42:03 | 200 | 28.706009875s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:42:55 | 200 | 14.469139167s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 21:48:41 | 200 |         5m23s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:01:35 | 200 |      51.292µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:01:35 | 200 |      131.25µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:01:45 | 200 |      22.083µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:01:45 | 200 |      20.542µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:01:56 | 200 |      30.875µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:01:56 | 200 |      29.208µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:02:05 | 200 |        28m17s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T22:02:06.774+05:30 level=WARN source=server.go:1269 msg="llama runner process no longer running" sys=9 string="signal: killed"
time=2026-02-07T22:02:06.774+05:30 level=WARN source=server.go:1269 msg="llama runner process no longer running" sys=9 string="signal: killed"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2026-02-07T22:02:06.805+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-07T22:02:06.806+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --port 59428"
time=2026-02-07T22:02:06.810+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="76.1 GiB" free_swap="0 B"
time=2026-02-07T22:02:06.810+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-07T22:02:06.810+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-07T22:02:06.811+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="2.0 GiB"
time=2026-02-07T22:02:06.811+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="48.0 GiB"
time=2026-02-07T22:02:06.811+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="8.0 GiB"
time=2026-02-07T22:02:06.811+05:30 level=INFO source=device.go:272 msg="total memory" size="58.0 GiB"
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-07T22:02:06.832+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.010 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-07T22:02:06.833+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-07T22:02:06.895+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:59428"
time=2026-02-07T22:02:06.898+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-07T22:02:06.899+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:02:06.899+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_embd_inp       = 3072
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 96
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 96
print_info: n_embd_head_v    = 96
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 3072
print_info: n_embd_v_gqa     = 3072
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 4096
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    52.84 MiB
load_tensors: Metal_Mapped model buffer size =  2021.82 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.13 MiB
llama_kv_cache:      Metal KV buffer size = 49152.00 MiB
[GIN] 2026/02/07 - 22:02:10 | 200 |      24.667µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:02:10 | 200 |       8.792µs |       127.0.0.1 | GET      "/api/ps"
llama_kv_cache: size = 49152.00 MiB (131072 cells,  32 layers,  1/1 seqs), K (f16): 24576.00 MiB, V (f16): 24576.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   396.01 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 935
llama_context: graph splits = 2
time=2026-02-07T22:02:11.170+05:30 level=INFO source=server.go:1387 msg="llama runner started in 4.36 seconds"
time=2026-02-07T22:02:11.170+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-07T22:02:11.170+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:02:11.170+05:30 level=INFO source=server.go:1387 msg="llama runner started in 4.36 seconds"
[GIN] 2026/02/07 - 22:02:21 | 200 |      25.125µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:02:21 | 200 |       22.75µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:02:31 | 200 |      44.875µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:02:31 | 200 |       18.75µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:02:39 | 200 |       19.75µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:02:39 | 200 |      26.708µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:02:46 | 200 |      17.708µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:02:46 | 200 |     15.7835ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/07 - 22:02:46 | 200 |     752.959µs |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/02/07 - 22:02:47 | 200 | 40.302474458s |       127.0.0.1 | POST     "/api/chat"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2026-02-07T22:02:48.115+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-07T22:02:48.116+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --port 59466"
time=2026-02-07T22:02:48.120+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="73.9 GiB" free_swap="0 B"
time=2026-02-07T22:02:48.120+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-07T22:02:48.120+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-07T22:02:48.121+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="2.0 GiB"
time=2026-02-07T22:02:48.121+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="48.0 GiB"
time=2026-02-07T22:02:48.121+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="8.0 GiB"
time=2026-02-07T22:02:48.121+05:30 level=INFO source=device.go:272 msg="total memory" size="58.0 GiB"
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-07T22:02:48.138+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.009 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-07T22:02:48.139+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-07T22:02:48.196+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:59466"
time=2026-02-07T22:02:48.197+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-07T22:02:48.197+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:02:48.198+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_embd_inp       = 3072
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 96
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 96
print_info: n_embd_head_v    = 96
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 3072
print_info: n_embd_v_gqa     = 3072
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 4096
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    52.84 MiB
load_tensors: Metal_Mapped model buffer size =  2021.82 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.13 MiB
[GIN] 2026/02/07 - 22:02:48 | 200 |      35.875µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:02:48 | 200 |      12.792µs |       127.0.0.1 | GET      "/api/ps"
llama_kv_cache:      Metal KV buffer size = 49152.00 MiB
llama_kv_cache: size = 49152.00 MiB (131072 cells,  32 layers,  1/1 seqs), K (f16): 24576.00 MiB, V (f16): 24576.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   396.01 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 935
llama_context: graph splits = 2
time=2026-02-07T22:02:52.466+05:30 level=INFO source=server.go:1387 msg="llama runner started in 4.35 seconds"
time=2026-02-07T22:02:52.466+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-07T22:02:52.466+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:02:52.466+05:30 level=INFO source=server.go:1387 msg="llama runner started in 4.35 seconds"
[GIN] 2026/02/07 - 22:02:56 | 200 |      18.834µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:02:56 | 200 |      21.041µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:03:07 | 200 |      22.875µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:03:07 | 200 |      28.458µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:03:16 | 200 |      43.042µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:03:16 | 200 |   30.860167ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/07 - 22:03:16 | 200 |    1.346791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/02/07 - 22:03:16 | 200 | 28.916043458s |       127.0.0.1 | POST     "/api/chat"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2026-02-07T22:03:18.036+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-07T22:03:18.036+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --port 59487"
time=2026-02-07T22:03:18.040+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="74.8 GiB" free_swap="0 B"
time=2026-02-07T22:03:18.040+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-07T22:03:18.040+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-07T22:03:18.041+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="2.0 GiB"
time=2026-02-07T22:03:18.041+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="48.0 GiB"
time=2026-02-07T22:03:18.041+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="8.0 GiB"
time=2026-02-07T22:03:18.041+05:30 level=INFO source=device.go:272 msg="total memory" size="58.0 GiB"
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-07T22:03:18.059+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.009 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-07T22:03:18.059+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-07T22:03:18.119+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:59487"
time=2026-02-07T22:03:18.119+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-07T22:03:18.119+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:03:18.119+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_embd_inp       = 3072
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 96
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 96
print_info: n_embd_head_v    = 96
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 3072
print_info: n_embd_v_gqa     = 3072
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 4096
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    52.84 MiB
load_tensors: Metal_Mapped model buffer size =  2021.82 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.13 MiB
llama_kv_cache:      Metal KV buffer size = 49152.00 MiB
llama_kv_cache: size = 49152.00 MiB (131072 cells,  32 layers,  1/1 seqs), K (f16): 24576.00 MiB, V (f16): 24576.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   396.01 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 935
llama_context: graph splits = 2
time=2026-02-07T22:03:22.391+05:30 level=INFO source=server.go:1387 msg="llama runner started in 4.35 seconds"
time=2026-02-07T22:03:22.391+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-07T22:03:22.391+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:03:22.391+05:30 level=INFO source=server.go:1387 msg="llama runner started in 4.35 seconds"
[GIN] 2026/02/07 - 22:03:26 | 200 |      22.375µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:03:26 | 200 |      19.875µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:03:34 | 200 |      19.458µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:03:34 | 200 |   14.710042ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/07 - 22:03:34 | 200 |     862.958µs |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/02/07 - 22:03:34 | 200 | 16.341527125s |       127.0.0.1 | POST     "/api/chat"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2026-02-07T22:03:35.384+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-07T22:03:35.384+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --port 59507"
time=2026-02-07T22:03:35.388+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="74.2 GiB" free_swap="0 B"
time=2026-02-07T22:03:35.388+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-07T22:03:35.388+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-07T22:03:35.389+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="2.0 GiB"
time=2026-02-07T22:03:35.389+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="48.0 GiB"
time=2026-02-07T22:03:35.389+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="8.0 GiB"
time=2026-02-07T22:03:35.389+05:30 level=INFO source=device.go:272 msg="total memory" size="58.0 GiB"
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-07T22:03:35.405+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.009 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-07T22:03:35.406+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-07T22:03:35.457+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:59507"
time=2026-02-07T22:03:35.465+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-07T22:03:35.465+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:03:35.465+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_embd_inp       = 3072
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 96
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 96
print_info: n_embd_head_v    = 96
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 3072
print_info: n_embd_v_gqa     = 3072
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 4096
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    52.84 MiB
load_tensors: Metal_Mapped model buffer size =  2021.82 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.13 MiB
llama_kv_cache:      Metal KV buffer size = 49152.00 MiB
llama_kv_cache: size = 49152.00 MiB (131072 cells,  32 layers,  1/1 seqs), K (f16): 24576.00 MiB, V (f16): 24576.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   396.01 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 935
llama_context: graph splits = 2
time=2026-02-07T22:03:39.744+05:30 level=INFO source=server.go:1387 msg="llama runner started in 4.36 seconds"
time=2026-02-07T22:03:39.744+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-07T22:03:39.744+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:03:39.744+05:30 level=INFO source=server.go:1387 msg="llama runner started in 4.36 seconds"
[GIN] 2026/02/07 - 22:03:56 | 200 | 21.338896291s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:04:49 | 200 | 52.180964042s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:05:02 | 200 | 12.561403625s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:05:13 | 200 |   10.0965645s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:05:17 | 200 |  3.948484417s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:05:22 | 200 |  3.941117791s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:05:26 | 200 |  2.142822708s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:05:29 | 200 |  2.068962042s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:05:31 | 200 |  1.121040625s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:05:34 | 200 |  2.832967084s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:05:56 | 200 | 22.431069667s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:06:20 | 200 | 22.613306292s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:06:43 | 200 |      41.084µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:06:43 | 200 |   14.338917ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/07 - 22:06:43 | 200 |     912.334µs |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/02/07 - 22:06:47 | 200 | 25.751286375s |       127.0.0.1 | POST     "/api/chat"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2026-02-07T22:06:48.624+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-07T22:06:48.625+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --port 59591"
time=2026-02-07T22:06:48.628+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="95.2 GiB" free_swap="0 B"
time=2026-02-07T22:06:48.628+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-07T22:06:48.628+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-07T22:06:48.629+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="2.0 GiB"
time=2026-02-07T22:06:48.629+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="48.0 GiB"
time=2026-02-07T22:06:48.629+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="8.0 GiB"
time=2026-02-07T22:06:48.629+05:30 level=INFO source=device.go:272 msg="total memory" size="58.0 GiB"
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-07T22:06:48.646+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.010 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-07T22:06:48.646+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-07T22:06:48.698+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:59591"
time=2026-02-07T22:06:48.705+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-07T22:06:48.706+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:06:48.706+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_embd_inp       = 3072
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 96
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 96
print_info: n_embd_head_v    = 96
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 3072
print_info: n_embd_v_gqa     = 3072
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 4096
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    52.84 MiB
load_tensors: Metal_Mapped model buffer size =  2021.82 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.13 MiB
[GIN] 2026/02/07 - 22:06:49 | 200 |      21.417µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:06:49 | 200 |       8.458µs |       127.0.0.1 | GET      "/api/ps"
llama_kv_cache:      Metal KV buffer size = 49152.00 MiB
llama_kv_cache: size = 49152.00 MiB (131072 cells,  32 layers,  1/1 seqs), K (f16): 24576.00 MiB, V (f16): 24576.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   396.01 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 935
llama_context: graph splits = 2
time=2026-02-07T22:06:52.482+05:30 level=INFO source=server.go:1387 msg="llama runner started in 3.85 seconds"
time=2026-02-07T22:06:52.482+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-07T22:06:52.482+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:06:52.482+05:30 level=INFO source=server.go:1387 msg="llama runner started in 3.85 seconds"
time=2026-02-07T22:06:52.494+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="49.5 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-70B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 80
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type q4_0:  561 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 37.22 GiB (4.53 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta-Llama-3-70B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-07T22:06:52.888+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=8192
time=2026-02-07T22:06:52.889+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d --port 59602"
time=2026-02-07T22:06:52.896+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="43.1 GiB" free_swap="0 B"
time=2026-02-07T22:06:52.896+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="49.0 GiB" free="49.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-07T22:06:52.896+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=81 requested=-1
time=2026-02-07T22:06:52.899+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="36.7 GiB"
time=2026-02-07T22:06:52.899+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="2.5 GiB"
time=2026-02-07T22:06:52.899+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="1.1 GiB"
time=2026-02-07T22:06:52.899+05:30 level=INFO source=device.go:272 msg="total memory" size="40.2 GiB"
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-07T22:06:52.929+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.010 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-07T22:06:52.930+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-07T22:06:52.996+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:59602"
time=2026-02-07T22:06:52.998+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:8192 KvCacheType: NumThreads:12 GPULayers:81[ID:0 Layers:81(0..80)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-07T22:06:52.999+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:06:52.999+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-70B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 80
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type q4_0:  561 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 37.22 GiB (4.53 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 8192
print_info: n_embd_inp       = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta-Llama-3-70B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
[GIN] 2026/02/07 - 22:07:21 | 200 |     164.083µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:07:21 | 200 |      36.333µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:07:28 | 200 |      35.042µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:07:28 | 200 |   21.559916ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/07 - 22:07:28 | 200 |    1.068625ms |       127.0.0.1 | POST     "/api/generate"
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
load_tensors: Metal_Mapped model buffer size = 37546.98 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 8192
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.52 MiB
llama_kv_cache:      Metal KV buffer size =  2560.00 MiB
llama_kv_cache: size = 2560.00 MiB (  8192 cells,  80 layers,  1/1 seqs), K (f16): 1280.00 MiB, V (f16): 1280.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   266.50 MiB
llama_context:        CPU compute buffer size =    32.01 MiB
llama_context: graph nodes  = 2487
llama_context: graph splits = 2
time=2026-02-07T22:07:38.250+05:30 level=INFO source=server.go:1387 msg="llama runner started in 45.35 seconds"
time=2026-02-07T22:07:38.250+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=2
time=2026-02-07T22:07:38.250+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:07:38.250+05:30 level=INFO source=server.go:1387 msg="llama runner started in 45.35 seconds"
[GIN] 2026/02/07 - 22:07:40 | 200 |      54.542µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:07:40 | 200 |     101.625µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:08:03 | 200 |      50.875µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:08:03 | 200 |      69.459µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:08:11 | 200 |      20.916µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:08:11 | 200 |     100.625µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:08:22 | 200 |       26.75µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:08:22 | 200 |      92.458µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:08:40 | 200 |         1m51s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T22:08:41.814+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="67.3 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2026-02-07T22:08:41.865+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-07T22:08:41.866+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --port 59660"
time=2026-02-07T22:08:41.874+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="56.1 GiB" free_swap="0 B"
time=2026-02-07T22:08:41.874+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="66.8 GiB" free="67.3 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-07T22:08:41.874+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-07T22:08:41.875+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="2.0 GiB"
time=2026-02-07T22:08:41.875+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="48.0 GiB"
time=2026-02-07T22:08:41.875+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="8.0 GiB"
time=2026-02-07T22:08:41.875+05:30 level=INFO source=device.go:272 msg="total memory" size="58.0 GiB"
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-07T22:08:41.923+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.036 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-07T22:08:41.925+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-07T22:08:42.127+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:59660"
time=2026-02-07T22:08:42.132+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-07T22:08:42.132+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:08:42.133+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_embd_inp       = 3072
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 96
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 96
print_info: n_embd_head_v    = 96
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 3072
print_info: n_embd_v_gqa     = 3072
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 4096
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    52.84 MiB
load_tensors: Metal_Mapped model buffer size =  2021.82 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.13 MiB
[GIN] 2026/02/07 - 22:08:43 | 200 |      24.666µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:08:43 | 200 |      28.666µs |       127.0.0.1 | GET      "/api/ps"
llama_kv_cache:      Metal KV buffer size = 49152.00 MiB
llama_kv_cache: size = 49152.00 MiB (131072 cells,  32 layers,  1/1 seqs), K (f16): 24576.00 MiB, V (f16): 24576.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   396.01 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 935
llama_context: graph splits = 2
time=2026-02-07T22:08:46.654+05:30 level=INFO source=server.go:1387 msg="llama runner started in 4.78 seconds"
time=2026-02-07T22:08:46.654+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=2
time=2026-02-07T22:08:46.654+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:08:46.654+05:30 level=INFO source=server.go:1387 msg="llama runner started in 4.78 seconds"
[GIN] 2026/02/07 - 22:08:52 | 200 |      23.333µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:08:52 | 200 |      36.708µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:08:58 | 200 |      21.709µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:08:58 | 200 |   17.662416ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/07 - 22:08:58 | 200 |     872.666µs |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/02/07 - 22:08:58 | 200 |      27.334µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:08:58 | 200 |      54.917µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:09:05 | 200 |      18.583µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:09:05 | 200 |      17.666µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:09:11 | 200 |      35.333µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:09:11 | 200 |      44.083µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:09:17 | 200 |       20.75µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:09:17 | 200 |       36.25µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:09:48 | 200 |          1m6s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T22:09:49.771+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="67.3 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2026-02-07T22:09:49.798+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-07T22:09:49.799+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --port 59702"
time=2026-02-07T22:09:49.802+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="57.3 GiB" free_swap="0 B"
time=2026-02-07T22:09:49.802+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="66.8 GiB" free="67.3 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-07T22:09:49.802+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-07T22:09:49.803+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="2.0 GiB"
time=2026-02-07T22:09:49.803+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="48.0 GiB"
time=2026-02-07T22:09:49.803+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="8.0 GiB"
time=2026-02-07T22:09:49.803+05:30 level=INFO source=device.go:272 msg="total memory" size="58.0 GiB"
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-07T22:09:49.830+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.011 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-07T22:09:49.831+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-07T22:09:49.892+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:59702"
time=2026-02-07T22:09:49.903+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-07T22:09:49.904+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:09:49.904+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_embd_inp       = 3072
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 96
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 96
print_info: n_embd_head_v    = 96
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 3072
print_info: n_embd_v_gqa     = 3072
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 4096
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    52.84 MiB
load_tensors: Metal_Mapped model buffer size =  2021.82 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.13 MiB
llama_kv_cache:      Metal KV buffer size = 49152.00 MiB
llama_kv_cache: size = 49152.00 MiB (131072 cells,  32 layers,  1/1 seqs), K (f16): 24576.00 MiB, V (f16): 24576.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   396.01 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 935
llama_context: graph splits = 2
time=2026-02-07T22:09:54.677+05:30 level=INFO source=server.go:1387 msg="llama runner started in 4.87 seconds"
time=2026-02-07T22:09:54.677+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=2
time=2026-02-07T22:09:54.677+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:09:54.677+05:30 level=INFO source=server.go:1387 msg="llama runner started in 4.87 seconds"
[GIN] 2026/02/07 - 22:10:54 | 200 |          1m4s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:13:45 | 200 |      29.875µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:13:45 | 200 |       36.25µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:13:50 | 200 |      26.708µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:13:50 | 200 |   16.063375ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/07 - 22:13:50 | 200 |     783.916µs |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/02/07 - 22:13:57 | 200 |       18.25µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:13:57 | 200 |      18.875µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:14:07 | 200 |      21.291µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:14:07 | 200 |      27.084µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:14:16 | 200 |         3m20s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T22:14:17.402+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="67.3 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2026-02-07T22:14:17.429+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-07T22:14:17.430+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --port 59772"
time=2026-02-07T22:14:17.436+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="60.8 GiB" free_swap="0 B"
time=2026-02-07T22:14:17.436+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="66.8 GiB" free="67.3 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-07T22:14:17.436+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-07T22:14:17.437+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="2.0 GiB"
time=2026-02-07T22:14:17.437+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="48.0 GiB"
time=2026-02-07T22:14:17.437+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="8.0 GiB"
time=2026-02-07T22:14:17.437+05:30 level=INFO source=device.go:272 msg="total memory" size="58.0 GiB"
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-07T22:14:17.455+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.011 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-07T22:14:17.456+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-07T22:14:17.526+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:59772"
time=2026-02-07T22:14:17.529+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-07T22:14:17.529+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:14:17.529+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_embd_inp       = 3072
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 96
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 96
print_info: n_embd_head_v    = 96
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 3072
print_info: n_embd_v_gqa     = 3072
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 4096
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    52.84 MiB
load_tensors: Metal_Mapped model buffer size =  2021.82 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.13 MiB
[GIN] 2026/02/07 - 22:14:18 | 200 |         7m28s |       127.0.0.1 | POST     "/api/chat"
llama_kv_cache:      Metal KV buffer size = 49152.00 MiB
llama_kv_cache: size = 49152.00 MiB (131072 cells,  32 layers,  1/1 seqs), K (f16): 24576.00 MiB, V (f16): 24576.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   396.01 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 935
llama_context: graph splits = 2
time=2026-02-07T22:14:22.313+05:30 level=INFO source=server.go:1387 msg="llama runner started in 4.88 seconds"
time=2026-02-07T22:14:22.313+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=2
time=2026-02-07T22:14:22.313+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T22:14:22.313+05:30 level=INFO source=server.go:1387 msg="llama runner started in 4.88 seconds"
[GIN] 2026/02/07 - 22:15:41 | 200 |         1m23s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:15:44 | 200 |       22.75µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:15:44 | 200 |      34.208µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 22:15:51 | 200 |  8.758131917s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:16:01 | 200 |  8.785310375s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:16:30 | 200 | 28.003627625s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:16:59 | 200 |    28.283286s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:17:08 | 200 |  7.525693167s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:17:16 | 200 |  6.775743208s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:17:33 | 200 | 16.575237958s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:17:50 | 200 |   15.3368155s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:18:03 | 200 |          33µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:18:03 | 200 |    1.285125ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2026/02/07 - 22:18:04 | 200 | 12.708236416s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:18:10 | 200 |      41.875µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:18:10 | 200 |  133.669875ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/07 - 22:18:21 | 200 |      26.667µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:18:21 | 200 |  149.138958ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/07 - 22:18:35 | 200 | 30.197865959s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:19:00 | 200 |  49.34270525s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/02/07 - 22:19:00 | 200 |      26.667µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:19:00 | 200 |       561.5µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2026/02/07 - 22:19:07 | 200 |      47.916µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:19:08 | 200 | 31.950780417s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T22:19:17.519+05:30 level=INFO source=images.go:848 msg="request failed: Get \"https://registry.ollama.ai/v2/library/llama3.1/manifests/70b-instruct-q8_0\": net/http: TLS handshake timeout"
[GIN] 2026/02/07 - 22:19:17 | 200 |    10.409117s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2026/02/07 - 22:19:22 | 200 | 13.378920459s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:19:33 | 200 |  10.93402025s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:19:35 | 200 |      32.625µs |       127.0.0.1 | HEAD     "/"
time=2026-02-07T22:19:37.759+05:30 level=INFO source=download.go:179 msg="downloading e1394fca2f0d in 75 1 GB part(s)"
[GIN] 2026/02/07 - 22:19:45 | 200 |  10.49735525s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:20:03 | 200 |  16.50725125s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:20:19 | 200 | 15.690784166s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:20:32 | 200 | 11.336781333s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:20:44 | 200 | 11.193164833s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:20:51 | 200 |   6.15421125s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:20:58 | 200 |  5.487605209s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:21:11 | 200 | 12.340640667s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:21:21 | 200 |  9.439350625s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 22:21:30 | 200 |  8.697722375s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-07T22:30:59.139+05:30 level=INFO source=download.go:376 msg="e1394fca2f0d part 4 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2026-02-07T22:35:25.243+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 8 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2026-02-07T22:37:17.974+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 14 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2026-02-07T22:39:24.131+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
time=2026-02-07T22:39:24.131+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 22:39:24 | 500 | 30.001874792s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/02/07 - 22:39:24 | 500 | 30.001885167s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-07T22:39:24.135+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 22:39:24 | 500 | 30.001170458s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-07T22:39:24.762+05:30 level=INFO source=server.go:1567 msg="aborting completion request due to client closing the connection"
[GIN] 2026/02/07 - 22:39:24 | 500 | 30.003112042s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-07T22:40:17.836+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 16 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2026-02-07T22:40:17.975+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 11 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2026-02-07T22:40:19.027+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 17 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2026-02-07T22:43:23.977+05:30 level=INFO source=download.go:376 msg="e1394fca2f0d part 11 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2026-02-07T22:44:25.219+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 23 attempt 0 failed: read tcp [2409:4091:3f:f404:e867:828a:ff06:d180]:59904->[2606:4700:2ff9::1]:443: read: connection reset by peer, retrying in 1s"
time=2026-02-07T22:47:32.140+05:30 level=INFO source=download.go:376 msg="e1394fca2f0d part 28 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
[GIN] 2026/02/07 - 22:48:42 | 200 |      22.042µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 22:48:42 | 200 |      24.541µs |       127.0.0.1 | GET      "/api/ps"
time=2026-02-07T22:50:34.245+05:30 level=INFO source=download.go:376 msg="e1394fca2f0d part 25 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2026-02-07T22:54:44.142+05:30 level=INFO source=download.go:376 msg="e1394fca2f0d part 35 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2026-02-07T23:02:32.095+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 4 attempt 0 failed: unexpected EOF, retrying in 1s"
[GIN] 2026/02/07 - 23:02:48 | 200 |      31.167µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 23:02:48 | 200 |      31.208µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 23:02:54 | 200 |      29.541µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 23:02:54 | 200 |  123.657084ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/07 - 23:02:54 | 200 |    1.084166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/02/07 - 23:02:54 | 200 |      22.875µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 23:02:54 | 200 |    17.42825ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/07 - 23:02:54 | 200 |    1.085209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/02/07 - 23:02:54 | 200 |      33.209µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 23:02:54 | 200 |      30.375µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 23:03:09 | 200 |       26.25µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 23:03:09 | 200 |   88.912625ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/07 - 23:03:09 | 200 |     704.333µs |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/02/07 - 23:03:09 | 200 |      17.792µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 23:03:09 | 200 |      17.083µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 23:03:09 | 200 |        41m37s |       127.0.0.1 | POST     "/api/chat"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2026-02-07T23:03:10.553+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-07T23:03:10.553+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --port 60643"
time=2026-02-07T23:03:10.556+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="110.5 GiB" free_swap="0 B"
time=2026-02-07T23:03:10.556+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-07T23:03:10.556+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-07T23:03:10.557+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="2.0 GiB"
time=2026-02-07T23:03:10.557+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="48.0 GiB"
time=2026-02-07T23:03:10.557+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="8.0 GiB"
time=2026-02-07T23:03:10.557+05:30 level=INFO source=device.go:272 msg="total memory" size="58.0 GiB"
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-07T23:03:10.621+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.008 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-07T23:03:10.621+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-07T23:03:10.663+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:60643"
time=2026-02-07T23:03:10.672+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-07T23:03:10.673+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T23:03:10.673+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_embd_inp       = 3072
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 96
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 96
print_info: n_embd_head_v    = 96
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 3072
print_info: n_embd_v_gqa     = 3072
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 4096
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    52.84 MiB
load_tensors: Metal_Mapped model buffer size =  2021.82 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.13 MiB
llama_kv_cache:      Metal KV buffer size = 49152.00 MiB
llama_kv_cache: size = 49152.00 MiB (131072 cells,  32 layers,  1/1 seqs), K (f16): 24576.00 MiB, V (f16): 24576.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   396.01 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 935
llama_context: graph splits = 2
time=2026-02-07T23:03:13.930+05:30 level=INFO source=server.go:1387 msg="llama runner started in 3.37 seconds"
time=2026-02-07T23:03:13.930+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-07T23:03:13.930+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T23:03:13.930+05:30 level=INFO source=server.go:1387 msg="llama runner started in 3.37 seconds"
[GIN] 2026/02/07 - 23:06:12 | 200 |      24.875µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 23:06:12 | 200 |     567.333µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2026/02/07 - 23:06:28 | 200 |      29.333µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 23:06:28 | 200 |       26.25µs |       127.0.0.1 | GET      "/api/ps"
time=2026-02-07T23:06:32.085+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 27 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2026-02-07T23:06:35.396+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 21 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2026-02-07T23:06:38.251+05:30 level=INFO source=download.go:376 msg="e1394fca2f0d part 43 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
[GIN] 2026/02/07 - 23:10:42 | 200 |      28.209µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 23:10:42 | 200 |      17.011ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/07 - 23:10:42 | 200 |    1.035917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/02/07 - 23:10:42 | 200 |      19.416µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 23:10:42 | 200 |      20.083µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 23:10:49 | 200 |      40.584µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 23:10:49 | 200 |      43.209µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 23:10:49 | 200 |         7m38s |       127.0.0.1 | POST     "/api/chat"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2026-02-07T23:10:50.270+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-07T23:10:50.270+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --port 60788"
time=2026-02-07T23:10:50.273+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="107.7 GiB" free_swap="0 B"
time=2026-02-07T23:10:50.273+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-07T23:10:50.273+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-07T23:10:50.273+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="2.0 GiB"
time=2026-02-07T23:10:50.273+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="48.0 GiB"
time=2026-02-07T23:10:50.273+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="8.0 GiB"
time=2026-02-07T23:10:50.273+05:30 level=INFO source=device.go:272 msg="total memory" size="58.0 GiB"
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-07T23:10:50.293+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.006 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-07T23:10:50.293+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-07T23:10:50.329+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:60788"
time=2026-02-07T23:10:50.340+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-07T23:10:50.341+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T23:10:50.341+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_embd_inp       = 3072
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 96
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 96
print_info: n_embd_head_v    = 96
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 3072
print_info: n_embd_v_gqa     = 3072
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 4096
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    52.84 MiB
load_tensors: Metal_Mapped model buffer size =  2021.82 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.13 MiB
llama_kv_cache:      Metal KV buffer size = 49152.00 MiB
llama_kv_cache: size = 49152.00 MiB (131072 cells,  32 layers,  1/1 seqs), K (f16): 24576.00 MiB, V (f16): 24576.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   396.01 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 935
llama_context: graph splits = 2
time=2026-02-07T23:10:53.350+05:30 level=INFO source=server.go:1387 msg="llama runner started in 3.08 seconds"
time=2026-02-07T23:10:53.350+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-07T23:10:53.350+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-07T23:10:53.350+05:30 level=INFO source=server.go:1387 msg="llama runner started in 3.08 seconds"
[GIN] 2026/02/07 - 23:10:58 | 200 |  8.606732458s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 23:11:05 | 200 |  5.382546167s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 23:11:14 | 200 |  8.403241875s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 23:11:23 | 200 |  8.067080292s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 23:11:32 | 200 |  7.828553417s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 23:11:41 | 200 |  7.544194167s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 23:11:46 | 200 |  4.193956959s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 23:11:51 | 200 |  3.832444958s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 23:12:07 | 200 | 14.601701209s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 23:12:22 | 200 | 14.190698166s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 23:12:35 | 200 |    11.729526s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 23:12:48 | 200 | 11.395852291s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 23:12:56 | 200 |  7.751462875s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 23:13:05 | 200 |  7.520491667s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 23:13:13 | 200 |  6.566733708s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 23:13:20 | 200 |   6.25546925s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 23:13:46 | 200 |   25.1072405s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2026/02/07 - 23:18:22 | 200 |      27.084µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 23:18:22 | 200 |      30.209µs |       127.0.0.1 | GET      "/api/ps"
time=2026-02-07T23:18:32.180+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 7 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2026-02-07T23:20:23.731+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 7 attempt 1 failed: unexpected EOF, retrying in 2s"
time=2026-02-07T23:20:30.073+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 45 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2026-02-07T23:23:30.517+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 33 attempt 0 failed: unexpected EOF, retrying in 1s"
[GIN] 2026/02/07 - 23:25:32 | 200 |      31.042µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 23:25:32 | 200 |      24.042µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 23:26:28 | 200 |      35.708µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 23:26:28 | 200 |      41.583µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/07 - 23:28:07 | 200 |      29.792µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/07 - 23:28:07 | 200 |      27.041µs |       127.0.0.1 | GET      "/api/ps"
time=2026-02-07T23:29:43.173+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 32 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2026-02-07T23:30:37.477+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 55 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2026-02-07T23:31:18.754+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 16 attempt 1 failed: unexpected EOF, retrying in 2s"
time=2026-02-07T23:31:27.068+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 52 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2026-02-07T23:32:18.545+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 29 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2026-02-07T23:33:23.662+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 52 attempt 1 failed: unexpected EOF, retrying in 2s"
time=2026-02-07T23:33:38.430+05:30 level=INFO source=download.go:297 msg="e1394fca2f0d part 48 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2026-02-08T00:41:38.228+05:30 level=INFO source=download.go:179 msg="downloading 948af2743fc7 in 1 1.5 KB part(s)"
time=2026-02-08T00:41:40.092+05:30 level=INFO source=download.go:179 msg="downloading 0ba8f0e314b4 in 1 12 KB part(s)"
time=2026-02-08T00:41:41.979+05:30 level=INFO source=download.go:179 msg="downloading 56bb8bd477a5 in 1 96 B part(s)"
time=2026-02-08T00:41:44.403+05:30 level=INFO source=download.go:179 msg="downloading 1e618846a068 in 1 486 B part(s)"
[GIN] 2026/02/08 - 00:44:14 | 200 |      2h24m39s |       127.0.0.1 | POST     "/api/pull"
time=2026-02-08T00:48:30.362+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="49.5 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 7
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q8_0:  562 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 69.82 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-08T00:48:31.388+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-08T00:48:31.389+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 --port 62177"
time=2026-02-08T00:48:31.412+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="48.8 GiB" free_swap="0 B"
time=2026-02-08T00:48:31.412+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="49.0 GiB" free="49.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T00:48:31.412+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=81 requested=-1
time=2026-02-08T00:48:31.416+05:30 level=INFO source=server.go:1028 msg="model requires more gpu memory than is currently available, evicting a model to make space" "loaded layers"=35
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-08T00:48:31.517+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.051 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-08T00:48:31.518+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-08T00:48:31.707+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:62177"
[GIN] 2026/02/08 - 00:48:48 | 200 |      34.959µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 00:48:48 | 200 |      52.625µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 00:49:58 | 200 |      40.333µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 00:49:58 | 200 |     101.792µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 00:52:08 | 200 |      39.916µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 00:52:08 | 200 |        35.5µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 00:54:17 | 200 |      66.791µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 00:54:17 | 200 |      47.541µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 00:56:51 | 200 |      38.166µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 00:56:51 | 200 |      41.125µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:00:01 | 200 |      73.667µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:00:01 | 200 |         145µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:01:14 | 200 |        34.5µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:01:14 | 200 |      38.125µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:01:20 | 200 |      34.333µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:01:20 | 200 |      29.625µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:01:41 | 200 |      79.875µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:01:41 | 200 |    4.347083ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2026/02/08 - 01:01:41 | 200 |      38.833µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:01:41 | 200 |       30.25µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:01:50 | 200 |      33.125µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:01:50 | 200 |      35.083µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:01:50 | 200 |       1h48m3s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-08T01:01:50.687+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="48.0 GiB" free_swap="0 B"
time=2026-02-08T01:01:50.687+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T01:01:50.687+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=81 requested=-1
time=2026-02-08T01:01:50.688+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="55.9 GiB"
time=2026-02-08T01:01:50.688+05:30 level=INFO source=device.go:245 msg="model weights" device=CPU size="12.9 GiB"
time=2026-02-08T01:01:50.688+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="33.0 GiB"
time=2026-02-08T01:01:50.689+05:30 level=INFO source=device.go:256 msg="kv cache" device=CPU size="7.0 GiB"
time=2026-02-08T01:01:50.689+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="16.3 GiB"
time=2026-02-08T01:01:50.689+05:30 level=INFO source=device.go:272 msg="total memory" size="125.1 GiB"
time=2026-02-08T01:01:50.689+05:30 level=INFO source=sched.go:490 msg="Load failed" model=/Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 error="context canceled"
[GIN] 2026/02/08 - 01:01:50 | 499 |        13m20s |       127.0.0.1 | POST     "/api/generate"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 7
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q8_0:  562 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 69.82 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-08T01:01:50.935+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-08T01:01:50.935+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 --port 62406"
time=2026-02-08T01:01:50.940+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="62.7 GiB" free_swap="0 B"
time=2026-02-08T01:01:50.940+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T01:01:50.940+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=81 requested=-1
time=2026-02-08T01:01:50.941+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="55.9 GiB"
time=2026-02-08T01:01:50.941+05:30 level=INFO source=device.go:245 msg="model weights" device=CPU size="12.9 GiB"
time=2026-02-08T01:01:50.941+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="33.0 GiB"
time=2026-02-08T01:01:50.941+05:30 level=INFO source=device.go:256 msg="kv cache" device=CPU size="7.0 GiB"
time=2026-02-08T01:01:50.941+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="16.3 GiB"
time=2026-02-08T01:01:50.941+05:30 level=INFO source=device.go:272 msg="total memory" size="125.1 GiB"
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-08T01:01:51.011+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.010 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-08T01:01:51.011+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-08T01:01:51.067+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:62406"
time=2026-02-08T01:01:51.072+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:66[ID:0 Layers:66(14..79)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-08T01:01:51.072+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T01:01:51.072+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 7
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q8_0:  562 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 69.82 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_embd_inp       = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 66 repeating layers to GPU
load_tensors: offloaded 66/81 layers to GPU
load_tensors:   CPU_Mapped model buffer size = 71494.28 MiB
load_tensors: Metal_Mapped model buffer size = 69365.00 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.52 MiB
llama_kv_cache:        CPU KV buffer size =  7168.00 MiB
llama_kv_cache:      Metal KV buffer size = 33792.00 MiB
llama_kv_cache: size = 40960.00 MiB (131072 cells,  80 layers,  1/1 seqs), K (f16): 20480.00 MiB, V (f16): 20480.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   328.01 MiB
llama_context:        CPU compute buffer size =   448.01 MiB
llama_context: graph nodes  = 2487
llama_context: graph splits = 143 (with bs=512), 3 (with bs=1)
time=2026-02-08T01:02:08.902+05:30 level=INFO source=server.go:1387 msg="llama runner started in 17.96 seconds"
time=2026-02-08T01:02:08.902+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-08T01:02:08.902+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T01:02:08.903+05:30 level=INFO source=server.go:1387 msg="llama runner started in 17.96 seconds"
time=2026-02-08T01:02:08.909+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="2.3 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2026-02-08T01:02:08.939+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-08T01:02:08.940+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --port 62421"
time=2026-02-08T01:02:08.945+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="8.4 GiB" free_swap="0 B"
time=2026-02-08T01:02:08.945+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="1.8 GiB" free="2.3 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T01:02:08.945+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-08T01:02:08.945+05:30 level=INFO source=server.go:1028 msg="model requires more gpu memory than is currently available, evicting a model to make space" "loaded layers"=0
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-08T01:02:09.057+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.031 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-08T01:02:09.060+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-08T01:02:09.155+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:62421"
[GIN] 2026/02/08 - 01:02:56 | 200 |      85.292µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:02:56 | 200 |      97.667µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:05:11 | 200 |       48.75µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:05:11 | 200 |      92.417µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:07:17 | 200 |      37.875µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:07:17 | 200 |      63.583µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:09:25 | 200 |      28.833µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:09:25 | 200 |      52.792µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:11:33 | 200 |       73.25µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:11:33 | 200 |      63.667µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:13:39 | 200 |      97.667µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:14:43 | 500 |         15m0s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-08T01:14:43.694+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="75.3 GiB" free_swap="0 B"
time=2026-02-08T01:14:43.694+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T01:14:43.694+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-08T01:14:43.694+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="2.0 GiB"
time=2026-02-08T01:14:43.694+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="48.0 GiB"
time=2026-02-08T01:14:43.694+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="8.0 GiB"
time=2026-02-08T01:14:43.694+05:30 level=INFO source=device.go:272 msg="total memory" size="58.0 GiB"
time=2026-02-08T01:14:43.696+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2026-02-08T01:14:43.696+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-08T01:14:43.696+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_embd_inp       = 3072
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 96
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 96
print_info: n_embd_head_v    = 96
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 3072
print_info: n_embd_v_gqa     = 3072
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 4096
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    52.84 MiB
load_tensors: Metal_Mapped model buffer size =  2021.82 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.13 MiB
llama_kv_cache:      Metal KV buffer size = 49152.00 MiB
llama_kv_cache: size = 49152.00 MiB (131072 cells,  32 layers,  1/1 seqs), K (f16): 24576.00 MiB, V (f16): 24576.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   396.01 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 935
llama_context: graph splits = 2
time=2026-02-08T01:14:49.220+05:30 level=INFO source=server.go:1387 msg="llama runner started in 760.26 seconds"
time=2026-02-08T01:14:49.220+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-08T01:14:49.220+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T01:14:49.220+05:30 level=INFO source=server.go:1387 msg="llama runner started in 760.26 seconds"
time=2026-02-08T01:14:49.233+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="49.5 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 7
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q8_0:  562 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 69.82 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-08T01:14:49.474+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-08T01:14:49.475+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 --port 62619"
time=2026-02-08T01:14:49.480+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="58.4 GiB" free_swap="0 B"
time=2026-02-08T01:14:49.480+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="49.0 GiB" free="49.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T01:14:49.480+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=81 requested=-1
time=2026-02-08T01:14:49.481+05:30 level=INFO source=server.go:1028 msg="model requires more gpu memory than is currently available, evicting a model to make space" "loaded layers"=35
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-08T01:14:49.509+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.010 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-08T01:14:49.510+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-08T01:14:49.572+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:62619"
[GIN] 2026/02/08 - 01:14:52 | 200 |      26.125µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:14:52 | 200 |      35.208µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:15:00 | 200 |      40.542µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:15:00 | 200 |       24.25µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:15:07 | 200 |      25.334µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:15:07 | 200 |      26.042µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:15:25 | 200 |      23.625µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:15:25 | 200 |      28.541µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:15:37 | 200 |        32.5µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:15:37 | 200 |      18.375µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:17:44 | 200 |      28.083µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:17:44 | 200 |      32.792µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:17:50 | 200 |      23.209µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:17:50 | 200 |      27.708µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:17:59 | 200 |          44µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:17:59 | 200 |      34.667µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:18:05 | 200 |      27.042µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:18:05 | 200 |      19.792µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:18:06 | 200 |        16m14s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-08T01:18:06.345+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="55.8 GiB" free_swap="0 B"
time=2026-02-08T01:18:06.345+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T01:18:06.345+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=81 requested=-1
time=2026-02-08T01:18:06.346+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="55.9 GiB"
time=2026-02-08T01:18:06.346+05:30 level=INFO source=device.go:245 msg="model weights" device=CPU size="12.9 GiB"
time=2026-02-08T01:18:06.346+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="33.0 GiB"
time=2026-02-08T01:18:06.346+05:30 level=INFO source=device.go:256 msg="kv cache" device=CPU size="7.0 GiB"
time=2026-02-08T01:18:06.346+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="16.3 GiB"
time=2026-02-08T01:18:06.346+05:30 level=INFO source=device.go:272 msg="total memory" size="125.1 GiB"
time=2026-02-08T01:18:06.346+05:30 level=INFO source=sched.go:490 msg="Load failed" model=/Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 error="context canceled"
[GIN] 2026/02/08 - 01:18:06 | 499 |         3m23s |       127.0.0.1 | POST     "/api/generate"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 7
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q8_0:  562 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 69.82 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-08T01:18:06.600+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-08T01:18:06.601+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 --port 62677"
time=2026-02-08T01:18:06.606+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="60.9 GiB" free_swap="0 B"
time=2026-02-08T01:18:06.606+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T01:18:06.606+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=81 requested=-1
time=2026-02-08T01:18:06.607+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="55.9 GiB"
time=2026-02-08T01:18:06.607+05:30 level=INFO source=device.go:245 msg="model weights" device=CPU size="12.9 GiB"
time=2026-02-08T01:18:06.607+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="33.0 GiB"
time=2026-02-08T01:18:06.607+05:30 level=INFO source=device.go:256 msg="kv cache" device=CPU size="7.0 GiB"
time=2026-02-08T01:18:06.607+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="16.3 GiB"
time=2026-02-08T01:18:06.607+05:30 level=INFO source=device.go:272 msg="total memory" size="125.1 GiB"
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-08T01:18:06.622+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.012 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-08T01:18:06.623+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-08T01:18:06.681+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:62677"
time=2026-02-08T01:18:06.684+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:66[ID:0 Layers:66(14..79)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-08T01:18:06.684+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T01:18:06.684+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 7
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q8_0:  562 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 69.82 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_embd_inp       = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 66 repeating layers to GPU
load_tensors: offloaded 66/81 layers to GPU
load_tensors:   CPU_Mapped model buffer size = 71494.28 MiB
load_tensors: Metal_Mapped model buffer size = 69365.00 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.52 MiB
llama_kv_cache:        CPU KV buffer size =  7168.00 MiB
llama_kv_cache:      Metal KV buffer size = 33792.00 MiB
llama_kv_cache: size = 40960.00 MiB (131072 cells,  80 layers,  1/1 seqs), K (f16): 20480.00 MiB, V (f16): 20480.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   328.01 MiB
llama_context:        CPU compute buffer size =   448.01 MiB
llama_context: graph nodes  = 2487
llama_context: graph splits = 143 (with bs=512), 3 (with bs=1)
time=2026-02-08T01:18:21.250+05:30 level=INFO source=server.go:1387 msg="llama runner started in 14.64 seconds"
time=2026-02-08T01:18:21.250+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-08T01:18:21.250+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T01:18:21.251+05:30 level=INFO source=server.go:1387 msg="llama runner started in 14.64 seconds"
time=2026-02-08T01:18:21.254+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="2.3 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2026-02-08T01:18:21.283+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-08T01:18:21.284+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --port 62690"
time=2026-02-08T01:18:21.289+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="8.2 GiB" free_swap="0 B"
time=2026-02-08T01:18:21.290+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="1.8 GiB" free="2.3 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T01:18:21.290+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-08T01:18:21.290+05:30 level=INFO source=server.go:1028 msg="model requires more gpu memory than is currently available, evicting a model to make space" "loaded layers"=0
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-08T01:18:21.360+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.022 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-08T01:18:21.362+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-08T01:18:21.438+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:62690"
[GIN] 2026/02/08 - 01:23:13 | 200 |          22µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:23:13 | 200 |        71.5µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:28:21 | 200 |      29.292µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:28:21 | 200 |      46.417µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:33:29 | 200 |        52.5µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:33:29 | 200 |         105µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:38:37 | 200 |       33.25µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:38:37 | 200 |     132.917µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:43:44 | 200 |      26.041µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:43:44 | 200 |      67.375µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:44:51 | 200 |      34.167µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:44:51 | 200 |      46.459µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:45:11 | 200 |      26.125µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:45:11 | 200 |      37.791µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:45:16 | 500 |         30m0s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-08T01:45:16.646+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="8.0 GiB" free_swap="0 B"
time=2026-02-08T01:45:16.646+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T01:45:16.646+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-08T01:45:16.647+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="2.0 GiB"
time=2026-02-08T01:45:16.647+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="48.0 GiB"
time=2026-02-08T01:45:16.647+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="8.0 GiB"
time=2026-02-08T01:45:16.647+05:30 level=INFO source=device.go:272 msg="total memory" size="58.0 GiB"
time=2026-02-08T01:45:16.648+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2026-02-08T01:45:16.648+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-08T01:45:16.648+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_embd_inp       = 3072
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 96
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 96
print_info: n_embd_head_v    = 96
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 3072
print_info: n_embd_v_gqa     = 3072
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 4096
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    52.84 MiB
load_tensors: Metal_Mapped model buffer size =  2021.82 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.13 MiB
llama_kv_cache:      Metal KV buffer size = 49152.00 MiB
[GIN] 2026/02/08 - 01:45:21 | 200 |          22µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:45:21 | 200 |       9.542µs |       127.0.0.1 | GET      "/api/ps"
llama_kv_cache: size = 49152.00 MiB (131072 cells,  32 layers,  1/1 seqs), K (f16): 24576.00 MiB, V (f16): 24576.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   396.01 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 935
llama_context: graph splits = 2
time=2026-02-08T01:45:25.690+05:30 level=INFO source=server.go:1387 msg="llama runner started in 1624.39 seconds"
time=2026-02-08T01:45:25.690+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-08T01:45:25.690+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T01:45:25.691+05:30 level=INFO source=server.go:1387 msg="llama runner started in 1624.39 seconds"
time=2026-02-08T01:45:32.439+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="49.5 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 7
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q8_0:  562 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 69.82 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-08T01:45:32.692+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-08T01:45:32.692+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 --port 63086"
time=2026-02-08T01:45:32.698+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="7.4 GiB" free_swap="0 B"
time=2026-02-08T01:45:32.698+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="49.0 GiB" free="49.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T01:45:32.698+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=81 requested=-1
time=2026-02-08T01:45:32.699+05:30 level=INFO source=server.go:1028 msg="model requires more gpu memory than is currently available, evicting a model to make space" "loaded layers"=35
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-08T01:45:32.752+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.024 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-08T01:45:32.753+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-08T01:45:32.874+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:63086"
[GIN] 2026/02/08 - 01:45:53 | 200 |      38.541µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:45:53 | 200 |      64.958µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:46:00 | 200 |      54.042µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:46:00 | 200 |    1.976667ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2026/02/08 - 01:46:00 | 200 |      37.584µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:46:00 | 200 |      36.791µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 01:46:06 | 200 |        27m58s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-08T01:46:06.314+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="55.2 GiB" free_swap="0 B"
time=2026-02-08T01:46:06.314+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T01:46:06.314+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=81 requested=-1
time=2026-02-08T01:46:06.315+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="55.9 GiB"
time=2026-02-08T01:46:06.315+05:30 level=INFO source=device.go:245 msg="model weights" device=CPU size="12.9 GiB"
time=2026-02-08T01:46:06.315+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="33.0 GiB"
time=2026-02-08T01:46:06.315+05:30 level=INFO source=device.go:256 msg="kv cache" device=CPU size="7.0 GiB"
time=2026-02-08T01:46:06.315+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="16.3 GiB"
time=2026-02-08T01:46:06.315+05:30 level=INFO source=device.go:272 msg="total memory" size="125.1 GiB"
time=2026-02-08T01:46:06.316+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:66[ID:0 Layers:66(14..79)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-08T01:46:06.316+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T01:46:06.316+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 7
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q8_0:  562 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 69.82 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_embd_inp       = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 66 repeating layers to GPU
load_tensors: offloaded 66/81 layers to GPU
load_tensors:   CPU_Mapped model buffer size = 71494.28 MiB
load_tensors: Metal_Mapped model buffer size = 69365.00 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.52 MiB
llama_kv_cache:        CPU KV buffer size =  7168.00 MiB
llama_kv_cache:      Metal KV buffer size = 33792.00 MiB
llama_kv_cache: size = 40960.00 MiB (131072 cells,  80 layers,  1/1 seqs), K (f16): 20480.00 MiB, V (f16): 20480.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   328.01 MiB
llama_context:        CPU compute buffer size =   448.01 MiB
llama_context: graph nodes  = 2487
llama_context: graph splits = 143 (with bs=512), 3 (with bs=1)
time=2026-02-08T01:46:20.628+05:30 level=INFO source=server.go:1387 msg="llama runner started in 47.93 seconds"
time=2026-02-08T01:46:20.628+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-08T01:46:20.628+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T01:46:20.629+05:30 level=INFO source=server.go:1387 msg="llama runner started in 47.93 seconds"
time=2026-02-08T01:46:20.633+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="2.3 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2026-02-08T01:46:20.667+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-08T01:46:20.668+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --port 63111"
time=2026-02-08T01:46:20.672+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="7.3 GiB" free_swap="0 B"
time=2026-02-08T01:46:20.672+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="1.8 GiB" free="2.3 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T01:46:20.672+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-08T01:46:20.672+05:30 level=INFO source=server.go:1028 msg="model requires more gpu memory than is currently available, evicting a model to make space" "loaded layers"=0
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-08T01:46:20.733+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.025 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-08T01:46:20.735+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-08T01:46:20.820+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:63111"
[GIN] 2026/02/08 - 01:51:13 | 200 |       36.25µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 01:51:13 | 200 |     128.125µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 02:01:25 | 200 |      38.833µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 02:01:25 | 200 |      77.917µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 02:06:34 | 200 |      44.167µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 02:06:34 | 200 |     117.167µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 02:08:13 | 200 |     239.459µs |       127.0.0.1 | GET      "/api/version"
[GIN] 2026/02/08 - 02:08:14 | 200 |   19.432375ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2026/02/08 - 02:08:14 | 200 |      26.042µs |       127.0.0.1 | GET      "/api/version"
[GIN] 2026/02/08 - 02:08:14 | 200 |    3.477667ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2026/02/08 - 02:08:14 | 200 |   73.238375ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/08 - 02:08:14 | 200 |   39.292417ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/08 - 02:08:15 | 200 |  260.675583ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/08 - 02:08:15 | 200 |  215.922375ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/08 - 02:08:15 | 200 |  310.843125ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/08 - 02:08:15 | 200 |  242.709959ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2026/02/08 - 02:08:44 | 200 |      40.208µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 02:08:44 | 200 |      45.083µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 02:09:28 | 200 |        23m56s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-08T02:09:29.385+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="74.8 GiB" free_swap="0 B"
time=2026-02-08T02:09:29.385+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T02:09:29.385+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-08T02:09:29.385+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="2.0 GiB"
time=2026-02-08T02:09:29.385+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="48.0 GiB"
time=2026-02-08T02:09:29.385+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="8.0 GiB"
time=2026-02-08T02:09:29.385+05:30 level=INFO source=device.go:272 msg="total memory" size="58.0 GiB"
time=2026-02-08T02:09:29.386+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2026-02-08T02:09:29.386+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-08T02:09:29.387+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_embd_inp       = 3072
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 96
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 96
print_info: n_embd_head_v    = 96
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 3072
print_info: n_embd_v_gqa     = 3072
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 4096
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    52.84 MiB
load_tensors: Metal_Mapped model buffer size =  2021.82 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.13 MiB
llama_kv_cache:      Metal KV buffer size = 49152.00 MiB
llama_kv_cache: size = 49152.00 MiB (131072 cells,  32 layers,  1/1 seqs), K (f16): 24576.00 MiB, V (f16): 24576.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   396.01 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 935
llama_context: graph splits = 2
time=2026-02-08T02:09:33.403+05:30 level=INFO source=server.go:1387 msg="llama runner started in 1392.72 seconds"
time=2026-02-08T02:09:33.403+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-08T02:09:33.403+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T02:09:33.403+05:30 level=INFO source=server.go:1387 msg="llama runner started in 1392.72 seconds"
time=2026-02-08T02:09:33.412+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="49.5 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 7
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q8_0:  562 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 69.82 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-08T02:09:33.613+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-08T02:09:33.614+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 --port 63443"
time=2026-02-08T02:09:33.619+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="56.9 GiB" free_swap="0 B"
time=2026-02-08T02:09:33.619+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="49.0 GiB" free="49.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T02:09:33.619+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=81 requested=-1
time=2026-02-08T02:09:33.621+05:30 level=INFO source=server.go:1028 msg="model requires more gpu memory than is currently available, evicting a model to make space" "loaded layers"=35
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-08T02:09:33.646+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.008 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-08T02:09:33.647+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-08T02:09:33.700+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:63443"
[GIN] 2026/02/08 - 02:09:35 | 200 |      32.834µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 02:09:35 | 200 |      44.333µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 02:09:47 | 200 |        23m39s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-08T02:09:47.542+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="59.0 GiB" free_swap="0 B"
time=2026-02-08T02:09:47.542+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T02:09:47.542+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=81 requested=-1
time=2026-02-08T02:09:47.543+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="55.9 GiB"
time=2026-02-08T02:09:47.543+05:30 level=INFO source=device.go:245 msg="model weights" device=CPU size="12.9 GiB"
time=2026-02-08T02:09:47.543+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="33.0 GiB"
time=2026-02-08T02:09:47.543+05:30 level=INFO source=device.go:256 msg="kv cache" device=CPU size="7.0 GiB"
time=2026-02-08T02:09:47.543+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="16.3 GiB"
time=2026-02-08T02:09:47.543+05:30 level=INFO source=device.go:272 msg="total memory" size="125.1 GiB"
time=2026-02-08T02:09:47.544+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:66[ID:0 Layers:66(14..79)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-08T02:09:47.544+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T02:09:47.544+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 7
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q8_0:  562 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 69.82 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_embd_inp       = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 66 repeating layers to GPU
load_tensors: offloaded 66/81 layers to GPU
load_tensors:   CPU_Mapped model buffer size = 71494.28 MiB
load_tensors: Metal_Mapped model buffer size = 69365.00 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.52 MiB
llama_kv_cache:        CPU KV buffer size =  7168.00 MiB
llama_kv_cache:      Metal KV buffer size = 33792.00 MiB
llama_kv_cache: size = 40960.00 MiB (131072 cells,  80 layers,  1/1 seqs), K (f16): 20480.00 MiB, V (f16): 20480.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   328.01 MiB
llama_context:        CPU compute buffer size =   448.01 MiB
llama_context: graph nodes  = 2487
llama_context: graph splits = 143 (with bs=512), 3 (with bs=1)
time=2026-02-08T02:09:59.599+05:30 level=INFO source=server.go:1387 msg="llama runner started in 25.98 seconds"
time=2026-02-08T02:09:59.599+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-08T02:09:59.599+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T02:09:59.600+05:30 level=INFO source=server.go:1387 msg="llama runner started in 25.98 seconds"
time=2026-02-08T02:09:59.603+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="2.3 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2026-02-08T02:09:59.625+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-08T02:09:59.626+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --port 63455"
time=2026-02-08T02:09:59.632+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="7.4 GiB" free_swap="0 B"
time=2026-02-08T02:09:59.632+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="1.8 GiB" free="2.3 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T02:09:59.632+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-08T02:09:59.633+05:30 level=INFO source=server.go:1028 msg="model requires more gpu memory than is currently available, evicting a model to make space" "loaded layers"=0
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-08T02:09:59.674+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.023 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-08T02:09:59.675+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-08T02:09:59.739+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:63455"
[GIN] 2026/02/08 - 02:11:42 | 200 |      32.875µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 02:11:42 | 200 |      104.75µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 02:16:50 | 200 |      21.333µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 02:16:50 | 200 |      49.208µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 02:21:56 | 200 |      27.666µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 02:21:56 | 200 |      51.542µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 02:27:02 | 200 |      29.959µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 02:27:02 | 200 |      42.334µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 02:32:10 | 200 |      27.834µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 02:32:10 | 200 |      67.417µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 02:37:17 | 200 |      34.625µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 02:37:17 | 200 |     124.583µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 02:42:22 | 200 |      27.875µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 02:42:22 | 200 |      68.417µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 02:47:29 | 200 |      93.042µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 02:47:29 | 200 |     191.333µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 02:52:36 | 200 |      36.125µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 02:52:36 | 200 |      58.958µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 02:57:44 | 200 |      29.166µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 02:57:44 | 200 |      46.667µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 03:00:39 | 200 |        51m10s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-08T03:00:39.957+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="76.2 GiB" free_swap="0 B"
time=2026-02-08T03:00:39.957+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T03:00:39.957+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-08T03:00:39.958+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="2.0 GiB"
time=2026-02-08T03:00:39.958+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="48.0 GiB"
time=2026-02-08T03:00:39.958+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="8.0 GiB"
time=2026-02-08T03:00:39.958+05:30 level=INFO source=device.go:272 msg="total memory" size="58.0 GiB"
time=2026-02-08T03:00:39.962+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-08T03:00:39.963+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T03:00:39.964+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_embd_inp       = 3072
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 96
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 96
print_info: n_embd_head_v    = 96
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 3072
print_info: n_embd_v_gqa     = 3072
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 4096
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    52.84 MiB
load_tensors: Metal_Mapped model buffer size =  2021.82 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.13 MiB
llama_kv_cache:      Metal KV buffer size = 49152.00 MiB
llama_kv_cache: size = 49152.00 MiB (131072 cells,  32 layers,  1/1 seqs), K (f16): 24576.00 MiB, V (f16): 24576.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   396.01 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 935
llama_context: graph splits = 2
time=2026-02-08T03:00:43.981+05:30 level=INFO source=server.go:1387 msg="llama runner started in 3044.32 seconds"
time=2026-02-08T03:00:43.981+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-08T03:00:43.981+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T03:00:43.981+05:30 level=INFO source=server.go:1387 msg="llama runner started in 3044.32 seconds"
time=2026-02-08T03:00:43.991+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="49.5 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 7
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q8_0:  562 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 69.82 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-08T03:00:44.189+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-08T03:00:44.189+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 --port 64153"
time=2026-02-08T03:00:44.195+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="59.6 GiB" free_swap="0 B"
time=2026-02-08T03:00:44.195+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="49.0 GiB" free="49.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T03:00:44.195+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=81 requested=-1
time=2026-02-08T03:00:44.196+05:30 level=INFO source=server.go:1028 msg="model requires more gpu memory than is currently available, evicting a model to make space" "loaded layers"=35
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-08T03:00:44.220+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.009 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-08T03:00:44.220+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-08T03:00:44.271+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:64153"
[GIN] 2026/02/08 - 03:00:46 | 200 |      31.209µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 03:00:46 | 200 |      69.375µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 03:00:59 | 200 |        51m11s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-08T03:01:00.082+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="58.7 GiB" free_swap="0 B"
time=2026-02-08T03:01:00.082+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T03:01:00.082+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=81 requested=-1
time=2026-02-08T03:01:00.083+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="55.9 GiB"
time=2026-02-08T03:01:00.083+05:30 level=INFO source=device.go:245 msg="model weights" device=CPU size="12.9 GiB"
time=2026-02-08T03:01:00.083+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="33.0 GiB"
time=2026-02-08T03:01:00.083+05:30 level=INFO source=device.go:256 msg="kv cache" device=CPU size="7.0 GiB"
time=2026-02-08T03:01:00.083+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="16.3 GiB"
time=2026-02-08T03:01:00.083+05:30 level=INFO source=device.go:272 msg="total memory" size="125.1 GiB"
time=2026-02-08T03:01:00.083+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:66[ID:0 Layers:66(14..79)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-08T03:01:00.083+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T03:01:00.084+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 7
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q8_0:  562 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 69.82 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_embd_inp       = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 66 repeating layers to GPU
load_tensors: offloaded 66/81 layers to GPU
load_tensors:   CPU_Mapped model buffer size = 71494.28 MiB
load_tensors: Metal_Mapped model buffer size = 69365.00 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.52 MiB
llama_kv_cache:        CPU KV buffer size =  7168.00 MiB
llama_kv_cache:      Metal KV buffer size = 33792.00 MiB
llama_kv_cache: size = 40960.00 MiB (131072 cells,  80 layers,  1/1 seqs), K (f16): 20480.00 MiB, V (f16): 20480.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   328.01 MiB
llama_context:        CPU compute buffer size =   448.01 MiB
llama_context: graph nodes  = 2487
llama_context: graph splits = 143 (with bs=512), 3 (with bs=1)
time=2026-02-08T03:01:11.381+05:30 level=INFO source=server.go:1387 msg="llama runner started in 27.19 seconds"
time=2026-02-08T03:01:11.383+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-08T03:01:11.383+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T03:01:11.384+05:30 level=INFO source=server.go:1387 msg="llama runner started in 27.19 seconds"
time=2026-02-08T03:01:11.387+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="2.3 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2026-02-08T03:01:11.411+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-08T03:01:11.412+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --port 64159"
time=2026-02-08T03:01:11.422+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="8.1 GiB" free_swap="0 B"
time=2026-02-08T03:01:11.422+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="1.8 GiB" free="2.3 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T03:01:11.422+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-08T03:01:11.422+05:30 level=INFO source=server.go:1028 msg="model requires more gpu memory than is currently available, evicting a model to make space" "loaded layers"=0
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-08T03:01:11.446+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.022 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-08T03:01:11.447+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-08T03:01:11.530+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:64159"
[GIN] 2026/02/08 - 03:02:53 | 200 |      24.584µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 03:02:53 | 200 |      65.833µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 03:08:00 | 200 |      32.917µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 03:08:00 | 200 |       58.25µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 03:13:05 | 200 |      20.875µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 03:13:05 | 200 |       23.25µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 03:18:11 | 200 |      28.042µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 03:18:11 | 200 |      71.417µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 03:23:18 | 200 |      17.709µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 03:23:18 | 200 |       26.25µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 03:28:24 | 200 |        37.5µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 03:28:24 | 200 |      82.708µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 03:33:29 | 200 |      29.875µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 03:33:29 | 200 |       113.5µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 03:38:34 | 200 |      38.375µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 03:38:34 | 200 |      67.458µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 03:43:41 | 200 |       54.75µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 03:43:41 | 200 |      159.25µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 03:48:48 | 200 |       43.25µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 03:48:48 | 200 |     106.208µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 03:50:08 | 200 |        49m29s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-08T03:50:09.034+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="77.9 GiB" free_swap="0 B"
time=2026-02-08T03:50:09.034+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T03:50:09.034+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-08T03:50:09.035+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="2.0 GiB"
time=2026-02-08T03:50:09.035+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="48.0 GiB"
time=2026-02-08T03:50:09.035+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="8.0 GiB"
time=2026-02-08T03:50:09.035+05:30 level=INFO source=device.go:272 msg="total memory" size="58.0 GiB"
time=2026-02-08T03:50:09.036+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-08T03:50:09.037+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T03:50:09.037+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_embd_inp       = 3072
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 96
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 96
print_info: n_embd_head_v    = 96
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 3072
print_info: n_embd_v_gqa     = 3072
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 4096
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    52.84 MiB
load_tensors: Metal_Mapped model buffer size =  2021.82 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.13 MiB
llama_kv_cache:      Metal KV buffer size = 49152.00 MiB
llama_kv_cache: size = 49152.00 MiB (131072 cells,  32 layers,  1/1 seqs), K (f16): 24576.00 MiB, V (f16): 24576.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   396.01 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 935
llama_context: graph splits = 2
time=2026-02-08T03:50:12.804+05:30 level=INFO source=server.go:1387 msg="llama runner started in 2941.37 seconds"
time=2026-02-08T03:50:12.804+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-08T03:50:12.804+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T03:50:12.804+05:30 level=INFO source=server.go:1387 msg="llama runner started in 2941.37 seconds"
time=2026-02-08T03:50:12.814+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="49.5 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 7
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q8_0:  562 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 69.82 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-08T03:50:12.988+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-08T03:50:12.989+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 --port 64917"
time=2026-02-08T03:50:12.994+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="61.6 GiB" free_swap="0 B"
time=2026-02-08T03:50:12.994+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="49.0 GiB" free="49.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T03:50:12.994+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=81 requested=-1
time=2026-02-08T03:50:12.995+05:30 level=INFO source=server.go:1028 msg="model requires more gpu memory than is currently available, evicting a model to make space" "loaded layers"=35
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-08T03:50:13.024+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.008 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-08T03:50:13.026+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-08T03:50:13.072+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:64917"
[GIN] 2026/02/08 - 03:50:15 | 200 |        22.5µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 03:50:15 | 200 |      22.042µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 03:50:28 | 200 |        49m27s |       127.0.0.1 | POST     "/api/chat"
time=2026-02-08T03:50:28.427+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="61.1 GiB" free_swap="0 B"
time=2026-02-08T03:50:28.428+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="107.0 GiB" free="107.5 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T03:50:28.428+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=81 requested=-1
time=2026-02-08T03:50:28.428+05:30 level=INFO source=device.go:240 msg="model weights" device=Metal size="55.9 GiB"
time=2026-02-08T03:50:28.428+05:30 level=INFO source=device.go:245 msg="model weights" device=CPU size="12.9 GiB"
time=2026-02-08T03:50:28.428+05:30 level=INFO source=device.go:251 msg="kv cache" device=Metal size="33.0 GiB"
time=2026-02-08T03:50:28.428+05:30 level=INFO source=device.go:256 msg="kv cache" device=CPU size="7.0 GiB"
time=2026-02-08T03:50:28.428+05:30 level=INFO source=device.go:262 msg="compute graph" device=Metal size="16.3 GiB"
time=2026-02-08T03:50:28.428+05:30 level=INFO source=device.go:272 msg="total memory" size="125.1 GiB"
time=2026-02-08T03:50:28.429+05:30 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:131072 KvCacheType: NumThreads:12 GPULayers:66[ID:0 Layers:66(14..79)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
time=2026-02-08T03:50:28.429+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T03:50:28.429+05:30 level=INFO source=server.go:1383 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-e1394fca2f0d8147f867e4c0bc7d1cddeb122c4d0daf50fd9a874d182a88af85 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 7
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q8_0:  562 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 69.82 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_embd_inp       = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 66 repeating layers to GPU
load_tensors: offloaded 66/81 layers to GPU
load_tensors:   CPU_Mapped model buffer size = 71494.28 MiB
load_tensors: Metal_Mapped model buffer size = 69365.00 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_seq     = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M3 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.52 MiB
llama_kv_cache:        CPU KV buffer size =  7168.00 MiB
llama_kv_cache:      Metal KV buffer size = 33792.00 MiB
llama_kv_cache: size = 40960.00 MiB (131072 cells,  80 layers,  1/1 seqs), K (f16): 20480.00 MiB, V (f16): 20480.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   328.01 MiB
llama_context:        CPU compute buffer size =   448.01 MiB
llama_context: graph nodes  = 2487
llama_context: graph splits = 143 (with bs=512), 3 (with bs=1)
time=2026-02-08T03:50:39.226+05:30 level=INFO source=server.go:1387 msg="llama runner started in 26.23 seconds"
time=2026-02-08T03:50:39.226+05:30 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-08T03:50:39.226+05:30 level=INFO source=server.go:1349 msg="waiting for llama runner to start responding"
time=2026-02-08T03:50:39.226+05:30 level=INFO source=server.go:1387 msg="llama runner started in 26.23 seconds"
time=2026-02-08T03:50:39.231+05:30 level=INFO source=sched.go:655 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="107.5 GiB" available="2.3 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M3 Max) (unknown id) - 110100 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct
llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct
llama_model_loader: - kv   4:                           general.basename str              = Phi-3
llama_model_loader: - kv   5:                         general.size_label str              = mini
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072
llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  20:                          general.file_type u32              = 2
llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144
llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   67 tensors
llama_model_loader: - type q4_0:  129 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 2.03 GiB (4.55 BPW) 
load: printing all EOG tokens:
load:   - 32000 ('<|endoftext|>')
load:   - 32007 ('<|end|>')
load: special tokens cache size = 14
load: token to piece cache size = 0.1685 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 3.82 B
print_info: general.name     = Phi 3 Mini 128k Instruct
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2026-02-08T03:50:39.259+05:30 level=WARN source=server.go:168 msg="requested context size too large for model" num_ctx=262144 n_ctx_train=131072
time=2026-02-08T03:50:39.259+05:30 level=INFO source=server.go:430 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.15.5/bin/ollama runner --model /Users/rohitchandra/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --port 64928"
time=2026-02-08T03:50:39.263+05:30 level=INFO source=sched.go:463 msg="system memory" total="128.0 GiB" free="8.6 GiB" free_swap="0 B"
time=2026-02-08T03:50:39.263+05:30 level=INFO source=sched.go:470 msg="gpu memory" id=0 library=Metal available="1.8 GiB" free="2.3 GiB" minimum="512.0 MiB" overhead="0 B"
time=2026-02-08T03:50:39.263+05:30 level=INFO source=server.go:497 msg="loading model" "model layers"=33 requested=-1
time=2026-02-08T03:50:39.263+05:30 level=INFO source=server.go:1028 msg="model requires more gpu memory than is currently available, evicting a model to make space" "loaded layers"=0
MLX: Failed to load symbol: mlx_metal_device_info
time=2026-02-08T03:50:39.318+05:30 level=INFO source=runner.go:965 msg="starting go runner"
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.019 sec
ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)
ggml_metal_device_init: GPU name:   Apple M3 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 115448.73 MB
time=2026-02-08T03:50:39.318+05:30 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2026-02-08T03:50:39.376+05:30 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:64928"
[GIN] 2026/02/08 - 03:52:21 | 200 |      21.333µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 03:52:21 | 200 |      35.167µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 03:57:28 | 200 |      37.916µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 03:57:28 | 200 |      86.584µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 04:02:34 | 200 |      26.292µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 04:02:34 | 200 |      57.875µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2026/02/08 - 04:07:41 | 200 |      36.959µs |       127.0.0.1 | HEAD     "/"
[GIN] 2026/02/08 - 04:07:41 | 200 |      58.958µs |       127.0.0.1 | GET      "/api/ps"
